{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99487202-1e08-4508-b194-12026f705188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reading Data - Parquet Files\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Introduce the Parquet file format.\n",
    "- Read data from:\n",
    "  - Parquet files without a schema.\n",
    "  - Parquet files with a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756e37ba-9002-42ae-9281-df5366ab2ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Read CSV Data\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f6a8e8-727a-4937-b8b8-25217d5072ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from Parquet Files\n",
    "\n",
    "<strong style=\"font-size:larger\">\"</strong>Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.<strong style=\"font-size:larger\">\"</strong><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b445b9-6da2-4527-b62a-873323ebf718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### About Parquet Files\n",
    "* Free & Open Source.\n",
    "* Increased query performance over row-based data stores.\n",
    "* Provides efficient data compression.\n",
    "* Designed for performance on large data sets.\n",
    "* Supports limited schema evolution.\n",
    "* Is a splittable \"file format\".\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Row Format** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Column Format**\n",
    "\n",
    "<table style=\"border:0\">\n",
    "\n",
    "  <tr>\n",
    "    <th>ID</th><th>Name</th><th>Score</th>\n",
    "    <th style=\"border-top:0;border-bottom:0\">&nbsp;</th>\n",
    "    <th>ID:</th><td>1</td><td>2</td>\n",
    "    <td style=\"border-right: 1px solid #DDDDDD\">3</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>1</td><td>john</td><td>4.1</td>\n",
    "    <td style=\"border-top:0;border-bottom:0\">&nbsp;</td>\n",
    "    <th>Name:</th><td>john</td><td>mike</td>\n",
    "    <td style=\"border-right: 1px solid #DDDDDD\">sally</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>2</td><td>mike</td><td>3.5</td>\n",
    "    <td style=\"border-top:0;border-bottom:0\">&nbsp;</td>\n",
    "    <th style=\"border-bottom: 1px solid #DDDDDD\">Score:</th>\n",
    "    <td style=\"border-bottom: 1px solid #DDDDDD\">4.1</td>\n",
    "    <td style=\"border-bottom: 1px solid #DDDDDD\">3.5</td>\n",
    "    <td style=\"border-bottom: 1px solid #DDDDDD; border-right: 1px solid #DDDDDD\">6.4</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td style=\"border-bottom: 1px solid #DDDDDD\">3</td>\n",
    "    <td style=\"border-bottom: 1px solid #DDDDDD\">sally</td>\n",
    "    <td style=\"border-bottom: 1px solid #DDDDDD; border-right: 1px solid #DDDDDD\">6.4</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "See also\n",
    "* <a href=\"https://parquet.apache.org/\" target=\"_blank\">https&#58;//parquet.apache.org</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Apache_Parquet\" target=\"_blank\">https&#58;//en.wikipedia.org/wiki/Apache_Parquet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b75a2b-dd81-4529-b3ef-dc53c14925d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Source\n",
    "\n",
    "The data for this example shows the number of requests to Wikipedia's mobile and desktop websites from Wikipedia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2faf43e8-6284-4cfb-80cb-f49500abdda6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read in the Parquet Files\n",
    "\n",
    "To read in this files, we will specify the location of the parquet directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dfc3f59-3252-41af-9803-567efaf0b806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquetFile = \"../dataset/pageviews_by_second.parquet/\"\n",
    "\n",
    "(spark.read              # The DataFrameReader\n",
    "  .parquet(parquetFile)  # Creates a DataFrame from Parquet after reading in the file\n",
    "  .printSchema()         # Print the DataFrame's schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881c296e-082e-495f-ad89-ae3c7b884b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Review: Reading from Parquet Files\n",
    "* We do not need to specify the schema - the column names and data types are stored in the parquet files.\n",
    "* Only one job is required to **read** that schema from the parquet file's metadata.\n",
    "* Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b110b2-bc96-47e5-af23-c9698379191b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read in the Parquet Files w/Schema\n",
    "\n",
    "If you want to avoid the extra job entirely, we can, again, specify the schema even for parquet files:\n",
    "\n",
    "** *WARNING* ** *Providing a schema may avoid this one-time hit to determine the `DataFrame's` schema.*  \n",
    "*However, if you specify the wrong schema it will conflict with the true schema and will result in an analysis exception at runtime.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5265f331-ad0d-4587-837e-dbb03bc84877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parquetSchema = StructType(\n",
    "  [\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"site\", StringType(), False),\n",
    "    StructField(\"requests\", IntegerType(), False)\n",
    "  ]\n",
    ")\n",
    "\n",
    "(spark.read               # The DataFrameReader\n",
    "  .schema(parquetSchema)  # Use the specified schema\n",
    "  .parquet(parquetFile)   # Creates a DataFrame from Parquet after reading in the file\n",
    "  .printSchema()          # Print the DataFrame's schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e6be18d-4f98-4766-897a-b0b5d19f2503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab2106d6-d11f-41d7-ac05-1484bdc15285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquetDF = spark.read.schema(parquetSchema).parquet(parquetFile)\n",
    "\n",
    "print(\"Partitions: \" + str(parquetDF.rdd.getNumPartitions()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba48d16-b89a-4564-83a6-7841b6be2f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In most/many cases, people do not provide the schema for Parquet files because reading in the schema is such a cheap process.\n",
    "\n",
    "And lastly, let's peek at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc536d3-d9e2-4161-b5ee-a57aedd5acc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquetDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3.Reading Data - Parquet",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
